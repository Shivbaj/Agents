version: '3.8'

# Development Docker Compose configuration
# Use this for local development with hot reloading and debugging

services:
  # Main application (development mode)
  app:
    build: 
      context: .
      dockerfile: Dockerfile
      target: development  # We'll add multi-stage build
    ports:
      - "8000:8000"
    environment:
      # Development settings
      - ENVIRONMENT=development
      - DEBUG=true
      - API_HOST=0.0.0.0
      - API_PORT=8000
      
      # Database
      - DATABASE_URL=sqlite:///./data/agents.db
      
      # External services
      - REDIS_URL=redis://redis:6379
      - OLLAMA_BASE_URL=http://ollama:11434
      
      # API Keys (from .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY:-}
      
      # Development-specific
      - LOG_LEVEL=DEBUG
      - CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080", "http://localhost:8000"]
      
    volumes:
      # Mount source code for hot reloading
      - .:/app
      - ./data:/app/data
      - ./logs:/app/logs
      - dev_cache:/app/data/cache
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped
    networks:
      - agent-network
    command: >
      sh -c "
        uv run python scripts/dev_setup.py &&
        uv run uvicorn src.main:app 
        --reload 
        --host 0.0.0.0 
        --port 8000 
        --log-level debug
        --reload-dir src
      "

  # Redis for development
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_dev_data:/data
    networks:
      - agent-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru

  # Ollama for local LLM models
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_dev_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    networks:
      - agent-network
    restart: unless-stopped
    # Note: GPU support requires nvidia-docker runtime
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Development tools
  redis-commander:
    image: rediscommander/redis-commander
    ports:
      - "8081:8081"
    environment:
      - REDIS_HOSTS=redis:redis:6379
    networks:
      - agent-network
    depends_on:
      - redis
    profiles:
      - tools

  # Optional: Jupyter notebook for development
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      - .:/app
      - jupyter_data:/home/app/.jupyter
    networks:
      - agent-network
    command: >
      sh -c "
        uv add jupyter jupyterlab &&
        uv run jupyter lab 
        --ip=0.0.0.0 
        --port=8888 
        --no-browser 
        --allow-root
        --NotebookApp.token=''
        --NotebookApp.password=''
      "
    profiles:
      - tools

volumes:
  redis_dev_data:
  ollama_dev_data:
  dev_cache:
  jupyter_data:

networks:
  agent-network:
    driver: bridge