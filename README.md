# ü§ñ Multi-Agent System

A **clean, modular, and extensible** multi-agent system built with **FastAPI**, **LangChain**, **LangGraph**, and **Model Context Protocol (MCP)**. This system allows you to easily plug in new tools, servers, models, and APIs while maintaining a clean, object-oriented architecture.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.1+-purple.svg)](https://python.langchain.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ‚ú® Features

### üèóÔ∏è **Modular Architecture**
- **Object-oriented design** with clean interfaces
- **Plugin-based system** for easy extensibility
- **Base agent classes** for rapid development
- **Comprehensive error handling** and logging

### üß† **LangChain & LangGraph Integration**
- **Unified LLM interfaces** across providers
- **Advanced memory management** (conversation + vector storage)
- **Multi-agent workflows** with LangGraph
- **Prompt template system** with versioning

### üõ†Ô∏è **Tool Ecosystem**
- **Web search capabilities** (DuckDuckGo, Google)
- **File processing** (PDF, DOCX, TXT, images)
- **Safe code execution** (sandboxed Python)
- **Mathematical calculations**
- **Custom tool creation** framework

### üåê **Model Provider Support**
- **OpenAI** (GPT-4, GPT-3.5, DALL-E, Whisper)
- **Anthropic** (Claude 3)
- **Ollama** (Local models)
- **Easy provider extension**

### üßÆ **Advanced Memory**
- **Conversation buffer management**
- **Vector-based semantic search**
- **Persistent storage options**
- **Context-aware retrieval**

### üîÑ **Workflow Orchestration**
- **Multi-agent coordination** with LangGraph
- **Conditional routing** and decision making
- **Parallel execution** capabilities
- **Error recovery** and retry logic

---

## üöÄ Quick Start

### Prerequisites

- **Python 3.10+**
- **OpenAI API Key** (recommended)
- **Docker & Docker Compose** (optional)

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd multi-agent-system
```

2. **Install dependencies**
```bash
# Using pip
pip install -e .

# Or using uv (recommended)
pip install uv
uv sync
```

3. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

**Example .env file:**
```bash
# API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here

# Optional: Local model settings
OLLAMA_BASE_URL=http://localhost:11434

# Optional: Web search
GOOGLE_SEARCH_API_KEY=your_google_api_key
GOOGLE_SEARCH_ENGINE_ID=your_search_engine_id

# Database
DATABASE_URL=sqlite:///./data/agents.db

# Logging
LOG_LEVEL=INFO
```

4. **Run the quick start example**
```bash
python -m tests.examples.quickstart
```

5. **Start the FastAPI server**
```bash
# Development
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# Production
python -m src.main
```

6. **Test the API**
```bash
# Health check
curl http://localhost:8000/health

# Chat with an agent
curl -X POST "http://localhost:8000/api/v1/agents/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello! Can you help me calculate 25 * 17?",
    "agent_id": "general_assistant",
    "session_id": "test_session"
  }'
```

---

## üìÅ Project Structure

```
multi-agent-system/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 # ü§ñ Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base/              #   ‚îî‚îÄ‚îÄ Base agent classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ implementations/   #   ‚îî‚îÄ‚îÄ Concrete agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry/          #   ‚îî‚îÄ‚îÄ Agent discovery & management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ specialized/       #   ‚îî‚îÄ‚îÄ Domain-specific agents
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # üåê FastAPI routes & endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/               #   ‚îî‚îÄ‚îÄ API version 1
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config/                # ‚öôÔ∏è Configuration management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py       #   ‚îî‚îÄ‚îÄ Pydantic settings
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                  # üèõÔ∏è Core system components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py     #   ‚îî‚îÄ‚îÄ Exception handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py        #   ‚îî‚îÄ‚îÄ Logging setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py     #   ‚îî‚îÄ‚îÄ FastAPI middleware
‚îÇ   ‚îÇ

‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ memory/                # üß† Memory management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversation.py   #   ‚îî‚îÄ‚îÄ Conversation & vector memory
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ mcp/                   # üîå Model Context Protocol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base/             #   ‚îî‚îÄ‚îÄ MCP base classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ servers/          #   ‚îî‚îÄ‚îÄ MCP server implementations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/            #   ‚îî‚îÄ‚îÄ MCP tools
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                # ü§ñ LLM model providers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ providers/        #   ‚îî‚îÄ‚îÄ Provider implementations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ observability/         # üìä Monitoring & observability
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ langsmith.py      #   ‚îî‚îÄ‚îÄ LangSmith integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/          # üé≠ Multi-agent workflows
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflow.py       #   ‚îî‚îÄ‚îÄ LangGraph workflows
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/               # üìù Prompt management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager.py        #   ‚îî‚îÄ‚îÄ Template manager
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/        #   ‚îî‚îÄ‚îÄ Prompt templates (JSON)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/              # üîß Business logic services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_manager.py  #   ‚îî‚îÄ‚îÄ Model lifecycle management
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/                 # üõ†Ô∏è Agent tools & utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_tools.py     #   ‚îî‚îÄ‚îÄ Core tool implementations
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # üî® Utility functions
‚îÇ       ‚îî‚îÄ‚îÄ file_utils.py     #   ‚îî‚îÄ‚îÄ File operations
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # üß™ Test suite
‚îÇ   ‚îî‚îÄ‚îÄ examples/             # üìö Usage examples & demos
‚îú‚îÄ‚îÄ docker/                    # üê≥ Docker configurations
‚îú‚îÄ‚îÄ docs/                      # üìñ Documentation
‚îú‚îÄ‚îÄ .env.example              # üìÑ Environment template
‚îú‚îÄ‚îÄ pyproject.toml            # üì¶ Project configuration
‚îî‚îÄ‚îÄ README.md                 # üìã This file
```

---

## üéØ Core Concepts

### ü§ñ **Agents**

Agents are the core components that process messages and perform tasks. All agents inherit from `BaseAgent`:

```python
from src.agents.base.agent import BaseAgent
from src.agents.implementations.general_assistant_enhanced import GeneralAssistant

# Create an agent
agent = GeneralAssistant(
    agent_id="my_assistant",
    use_tools=True,
    use_web_search=True
)

await agent.initialize()

# Process messages
response = await agent.process_message(
    message="What's the weather like?",
    session_id="user_123"
)
```

### üß† **Memory System**

The memory system provides conversation context and semantic search:

```python
from src.memory.conversation import MemoryManager

# Create memory manager
memory_manager = MemoryManager()

# Conversation memory
conv_memory = memory_manager.create_conversation_memory("session_123")
await conv_memory.add_message("user", "I like Python programming")

# Hybrid memory (conversation + vector search)
hybrid_memory = memory_manager.create_hybrid_memory("session_456")
relevant_context = await hybrid_memory.search_relevant_context("programming")
```

### üìù **Prompt Templates**

Manage and version your prompts with the template system:

```python
from src.prompts.manager import PromptManager, create_agent_prompt

# Create prompt manager
prompt_manager = PromptManager()

# Create custom template
custom_prompt = create_agent_prompt(
    agent_name="Code Helper",
    task_description="Help with programming questions",
    personality_traits=["helpful", "technical", "patient"]
)

# Register and use
prompt_manager.register_template("code_helper", custom_prompt)
formatted_prompt = await prompt_manager.get_prompt("code_helper", {
    "user_input": "How do I create a Python class?"
})
```

### üõ†Ô∏è **Tools**

Tools extend agent capabilities:

```python
from src.tools.base_tools import get_tool

# Get a tool
calc_tool = get_tool("calculator")
result = await calc_tool._arun("(15 + 25) * 2")

# Web search
search_tool = get_tool("web_search")
results = await search_tool._arun("Python tutorials")

# File processing
file_tool = get_tool("file_processor")
content = await file_tool._arun("/path/to/document.pdf")
```

### üîÑ **Workflows**

Create complex multi-agent workflows with LangGraph:

```python
from src.orchestrator.workflow import WorkflowBuilder

# Create workflow
builder = WorkflowBuilder()
workflow = builder.create_research_workflow()

# Execute
result = await workflow.execute({
    "query": "Research AI developments in 2024"
})
```

---

## üèóÔ∏è Extending the System

### ü§ñ **Creating a Custom Agent**

```python
from src.agents.base.agent import BaseAgent, AgentResponse

class MyCustomAgent(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(
            name="My Custom Agent",
            description="A specialized agent for specific tasks",
            agent_type="custom",
            capabilities=["custom_task", "specialized_processing"],
            **kwargs
        )
    
    async def _initialize_agent(self):
        # Custom initialization logic
        pass
    
    async def _process_message(self, message: str, session_id: str, context: dict) -> AgentResponse:
        # Custom message processing logic
        response_content = f"Processed: {message}"
        
        return AgentResponse(
            content=response_content,
            metadata={"custom_field": "value"}
        )
```

### üõ†Ô∏è **Creating a Custom Tool**

```python
from langchain_core.tools import BaseTool

class MyCustomTool(BaseTool):
    name: str = "my_tool"
    description: str = "Description of what my tool does"
    
    def _run(self, input_data: str) -> str:
        # Synchronous implementation
        return f"Processed: {input_data}"
    
    async def _arun(self, input_data: str) -> str:
        # Asynchronous implementation
        return f"Processed: {input_data}"

# Register the tool
from src.tools.base_tools import AVAILABLE_TOOLS
AVAILABLE_TOOLS["my_tool"] = MyCustomTool
```

### ü§ñ **Adding a New Model Provider**

```python
from src.models.providers.base_provider import EnhancedBaseProvider

class MyModelProvider(EnhancedBaseProvider):
    def __init__(self, api_key: str, **kwargs):
        config = ProviderConfig(
            provider_name="my_provider",
            api_key=api_key,
            **kwargs
        )
        super().__init__(config)
    
    async def _initialize_provider(self):
        # Initialize connection to your model API
        pass
    
    def create_llm(self, model_name: str, **kwargs):
        # Create LangChain-compatible LLM instance
        pass
    
    async def list_models(self):
        # Return available models
        pass
```

### üìù **Creating Custom Prompt Templates**

Create JSON files in `src/prompts/templates/`:

```json
{
  "type": "chat",
  "metadata": {
    "id": "my_agent",
    "name": "My Custom Agent",
    "description": "A custom agent prompt",
    "version": "1.0.0",
    "variables": [
      {
        "name": "user_input",
        "type": "string",
        "required": true,
        "description": "User's input message"
      }
    ]
  },
  "system_message": "You are a helpful assistant specialized in...",
  "messages": [
    {
      "role": "human",
      "content": "{user_input}"
    }
  ]
}
```

### üîÑ **Creating Custom Workflows**

```python
from src.orchestrator.workflow import MultiAgentWorkflow, WorkflowNode

# Create custom workflow
workflow = MultiAgentWorkflow("my_workflow", "Custom Processing")

# Add nodes
workflow.add_node(WorkflowNode(
    name="validator",
    agent_type="general_assistant"
))

workflow.add_node(WorkflowNode(
    name="processor", 
    agent_type="specialized_agent"
))

workflow.add_node(WorkflowNode(
    name="formatter",
    function=my_custom_function
))

# Build workflow
workflow.compile()
workflow.add_edge("validator", "processor")
workflow.add_edge("processor", "formatter")
workflow.finish_at("formatter")

# Execute
result = await workflow.execute({"input": "data"})
```

---

## üîß Configuration

### ‚öôÔ∏è **Environment Variables**

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `OPENAI_API_KEY` | OpenAI API key | - | No* |
| `ANTHROPIC_API_KEY` | Anthropic API key | - | No |
| `OLLAMA_BASE_URL` | Ollama server URL | `http://localhost:11434` | No |
| `DATABASE_URL` | Database connection | `sqlite:///./data/agents.db` | No |
| `REDIS_URL` | Redis connection | `redis://localhost:6379` | No |
| `LOG_LEVEL` | Logging level | `INFO` | No |
| `DEBUG` | Debug mode | `False` | No |

*At least one model provider is recommended

### üìä **Model Configuration**

```python
# In src/config/settings.py
class Settings(BaseSettings):
    # OpenAI
    openai_api_key: Optional[str] = None
    openai_default_model: str = "gpt-4-turbo-preview"
    
    # Anthropic
    anthropic_api_key: Optional[str] = None
    anthropic_default_model: str = "claude-3-sonnet-20240229"
    
    # Ollama
    ollama_base_url: str = "http://localhost:11434"
    ollama_default_model: str = "llama3.2:1b"
```

---

## üê≥ Docker Deployment

### Development
```bash
docker-compose -f docker-compose.dev.yml up -d
```

### Production
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Custom Build
```bash
# Build image
docker build -t multi-agent-system .

# Run container
docker run -p 8000:8000 --env-file .env multi-agent-system
```

---

## üöÄ Git Deployment Guide

### Quick Deployment Checklist

1. **Clone the repository**
```bash
git clone <your-repository-url>
cd multi-agent-system
```

2. **Environment Setup**
```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys and configuration
nano .env  # or your preferred editor
```

3. **Install Dependencies**
```bash
# Using pip
pip install -e .

# Using uv (recommended)
pip install uv
uv sync
```

4. **Initialize Database** (if needed)
```bash
# Create data directory
mkdir -p data

# Run any database migrations
python -m src.scripts.init_db  # if you have initialization scripts
```

5. **Start the Application**
```bash
# Development
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# Production
python -m src.main
```

### üîß Production Deployment

#### Environment Variables for Production
Ensure these are set in your production `.env`:
```bash
# Security - CHANGE THESE!
SECRET_KEY=your-super-secure-secret-key-here
DEBUG=false
ENVIRONMENT=production

# Database (use production database)
DATABASE_URL=postgresql://user:password@localhost/dbname

# Redis (production instance)
REDIS_URL=redis://your-redis-server:6379

# API Keys (your actual keys)
OPENAI_API_KEY=your_actual_openai_key
ANTHROPIC_API_KEY=your_actual_anthropic_key

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/multi-agent-system/app.log
```

#### Using Docker in Production
```bash
# Build and run with docker-compose
docker-compose -f docker-compose.prod.yml up -d

# Or build and run manually
docker build -t multi-agent-system:latest .
docker run -d \
  --name multi-agent-system \
  -p 8000:8000 \
  --env-file .env \
  --restart unless-stopped \
  multi-agent-system:latest
```

#### Systemd Service (Linux Production)
Create `/etc/systemd/system/multi-agent-system.service`:
```ini
[Unit]
Description=Multi-Agent System API
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/opt/multi-agent-system
Environment=PATH=/opt/multi-agent-system/venv/bin
ExecStart=/opt/multi-agent-system/venv/bin/python -m src.main
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start:
```bash
sudo systemctl enable multi-agent-system
sudo systemctl start multi-agent-system
sudo systemctl status multi-agent-system
```

### üîí Security Considerations

**Before deploying to production:**

1. **Change default secrets**:
   - Generate new `SECRET_KEY`
   - Use production database credentials
   - Use production Redis instance

2. **Network security**:
   - Configure firewall rules
   - Use HTTPS/TLS certificates
   - Restrict API access if needed

3. **Environment isolation**:
   - Never commit `.env` files
   - Use environment-specific configurations
   - Rotate API keys regularly

### üìã Pre-Deployment Verification

Run this checklist before deploying:
```bash
# Test the application
python -m pytest

# Check for security issues
python -m bandit -r src/

# Lint the code
python -m flake8 src/
python -m black --check src/

# Test Docker build
docker build -t multi-agent-system-test .

# Test with your .env file
uvicorn src.main:app --host 0.0.0.0 --port 8000
curl http://localhost:8000/health
```

---

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test
pytest tests/test_agent_registry.py
```

---

## üìä Monitoring & Observability

### LangSmith Integration
```python
# Set environment variables
LANGSMITH_API_KEY=your_langsmith_key
LANGSMITH_PROJECT=multi-agent-system

# Automatic tracing is enabled for all LangChain operations
```

### Metrics
- Agent performance statistics
- Tool usage tracking
- Memory system metrics
- API endpoint monitoring

---

## ü§ù Contributing

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Commit changes** (`git commit -m 'Add amazing feature'`)
4. **Push to branch** (`git push origin feature/amazing-feature`)
5. **Open a Pull Request**

### Development Setup
```bash
# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run linting
black src tests
isort src tests
flake8 src tests
mypy src
```

---

## üìö Documentation

- **API Documentation**: `http://localhost:8000/docs` (when running)
- **Architecture Guide**: `docs/ARCHITECTURE.md`
- **Development Guide**: `docs/DEVELOPMENT.md`
- **API Examples**: `API_EXAMPLES.md`

---

## ‚ö†Ô∏è Important Notes

### üîí **Security**
- **Sandboxed code execution** for safety
- **Input validation** for all tools
- **API rate limiting** enabled
- **Environment isolation** recommended

### üéõÔ∏è **Performance**
- **Async/await** throughout the system
- **Connection pooling** for databases
- **Caching** for frequently used data
- **Memory optimization** for conversations

### üîÑ **Scalability**
- **Horizontal scaling** support
- **Load balancing** ready
- **Microservices architecture**
- **Database optimization**

---

## üêõ Troubleshooting

### Common Issues

**1. Import errors after installation**
```bash
# Reinstall in development mode
pip install -e .
```

**2. OpenAI API errors**
```bash
# Check API key
echo $OPENAI_API_KEY

# Verify connectivity
curl -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models
```

**3. Database connection issues**
```bash
# Create data directory
mkdir -p data

# Check permissions
ls -la data/
```

**4. Memory system issues**
```bash
# Install vector database dependencies
pip install chromadb faiss-cpu
```

---

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **[LangChain](https://python.langchain.com/)** - LLM framework
- **[LangGraph](https://langchain-ai.github.io/langgraph/)** - Multi-agent workflows
- **[FastAPI](https://fastapi.tiangolo.com/)** - Web framework
- **[Pydantic](https://docs.pydantic.dev/)** - Data validation
- **[ChromaDB](https://www.trychroma.com/)** - Vector database

---

## üöÄ Getting Started Checklist

- [ ] Install Python 3.10+
- [ ] Clone the repository
- [ ] Install dependencies (`pip install -e .`)
- [ ] Set up `.env` file with API keys
- [ ] Run quick start example (`python -m tests.examples.quickstart`)
- [ ] Start the API server (`uvicorn src.main:app --reload`)
- [ ] Test the API endpoints
- [ ] Explore the examples and documentation
- [ ] Create your first custom agent
- [ ] Build your first workflow

**Ready to build amazing AI agents? Let's get started! üéâ**